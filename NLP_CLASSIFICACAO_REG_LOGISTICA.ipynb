{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Using Machine Learning - Pyspark\n",
    "    - Objetivo: prever a classe de sentimento de qualquer crítica (positiva ou negativa). \n",
    "Dados: revisão do Movie Lens rotulados\n",
    "\n",
    "    - O dataframe apresenta as resenhas e classificação (label) positiva ou negativa\n",
    "    - Então, o algoritmo de ML tentará prever os novos casos como positivo ou negativo tomando como base as resenhas    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load DataFrame\n",
    "url='C:/Users/Pc/Desktop/Hartb/machineLearning/PYSPARK/Movie_reviews.txt'\n",
    "text_df = spark.read.csv(url,inferSchema=True,header=True,sep=',')\n",
    "\n",
    "text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+---------+\n",
      "|Review                                                                  |Sentiment|\n",
      "+------------------------------------------------------------------------+---------+\n",
      "|The Da Vinci Code book is just awesome.                                 |1        |\n",
      "|this was the first clive cussler i've ever read, but even books like Rel|1        |\n",
      "|i liked the Da Vinci Code a lot.                                        |1        |\n",
      "|i liked the Da Vinci Code a lot.                                        |1        |\n",
      "|I liked the Da Vinci Code but it ultimatly didn't seem to hold it's own.|1        |\n",
      "+------------------------------------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.show(5, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Você pode observar a coluna Sentiment em StringType, e precisaremos dela para convertê-la em um tipo Integer ou float daqui para frente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7087"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Temos cerca de sete mil registros dos quais alguns podem não estar devidamente rotulados. Portanto, filtramos apenas os registros rotulados corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6990"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = text_df.filter(((text_df.Sentiment =='1') | (text_df.Sentiment =='0')))\n",
    "\n",
    "text_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alguns dos registros foram filtrados e agora ficamos com 6.990 registros para análise. O próximo passo é validar uma série de revisões para cada classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Sentiment|count|\n",
      "+---------+-----+\n",
      "|        0| 3081|\n",
      "|        1| 3909|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.groupBy('Sentiment').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Estamos lidando com um conjunto de dados equilibrado aqui, pois ambas as classes têm um número quase semelhante de revisões. Vejamos alguns dos registros no conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+---------+\n",
      "|Review                                                                  |Sentiment|\n",
      "+------------------------------------------------------------------------+---------+\n",
      "|I loved the Harry Potter books before I discovered fanfiction but fanfic|1        |\n",
      "|This quiz sucks and Harry Potter sucks ok bye..                         |0        |\n",
      "|Brokeback Mountain is fucking horrible..                                |0        |\n",
      "|Brokeback Mountain is fucking horrible..                                |0        |\n",
      "|I love Harry Potter..                                                   |1        |\n",
      "|Mission Impossible 3 et al. can be said to be boring!                   |0        |\n",
      "|This quiz sucks and Harry Potter sucks ok bye..                         |0        |\n",
      "|Ok brokeback mountain is such a horrible movie.                         |0        |\n",
      "|The Da Vinci Code is awesome!!                                          |1        |\n",
      "|I love Brokeback Mountain.                                              |1        |\n",
      "+------------------------------------------------------------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.orderBy(rand()).show(10,False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Na próxima etapa, criamos uma nova coluna de rótulo como um tipo Integer e descartamos a coluna Sentiment original, que era do tipo String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+-----+\n",
      "|Review                                         |Label|\n",
      "+-----------------------------------------------+-----+\n",
      "|I love Harry Potter.                           |1.0  |\n",
      "|I LOVE BROKEBACK MOUNTAIN!                     |1.0  |\n",
      "|Gotta Love Harry Potter icons..                |1.0  |\n",
      "|mission impossible 2 rocks!!....               |1.0  |\n",
      "|I love Harry Potter.                           |1.0  |\n",
      "|This quiz sucks and Harry Potter sucks ok bye..|0.0  |\n",
      "|Brokeback Mountain was awesome.                |1.0  |\n",
      "|Mission Impossible 3 was AWESOME...            |1.0  |\n",
      "|I love Harry Potter.                           |1.0  |\n",
      "|So Brokeback Mountain was really depressing.   |0.0  |\n",
      "+-----------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df = text_df.withColumn(\"Label\", text_df.Sentiment.cast('float')).drop('Sentiment')\n",
    "\n",
    "text_df.orderBy(rand()).show(10,False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Também incluímos uma coluna adicional que captura o length da revisão.\n",
    "        - Ou seja conta o total de letras (incluindo espaço) no documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+-----+------+\n",
      "|Review                                                                  |Label|length|\n",
      "+------------------------------------------------------------------------+-----+------+\n",
      "|Brokeback Mountain is fucking horrible..                                |0.0  |40    |\n",
      "|Mission Impossible 3 was quite boring.                                  |0.0  |38    |\n",
      "|Harry Potter dragged Draco Malfoy ’ s trousers down past his hips and   |0.0  |69    |\n",
      "|I loved Brokeback Mountain and will check out the short stories one of t|1.0  |72    |\n",
      "|Brokeback Mountain was boring.                                          |0.0  |30    |\n",
      "|Not because I hate Harry Potter, but because I am the type of person tha|0.0  |72    |\n",
      "|I either LOVE Brokeback Mountain or think it's great that homosexuality |1.0  |71    |\n",
      "|I absolutely LOVE Harry Potter, as you can tell already.                |1.0  |56    |\n",
      "|I know you are all wondering now about whom I love more � Harry Potter  |1.0  |70    |\n",
      "|dudeee i LOVED brokeback mountain!!!!                                   |1.0  |37    |\n",
      "+------------------------------------------------------------------------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df = text_df.withColumn('length',length(text_df['Review']))\n",
    "\n",
    "text_df.orderBy(rand()).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Label|      avg(Length)|\n",
      "+-----+-----------------+\n",
      "|  1.0|47.61882834484523|\n",
      "|  0.0|50.95845504706264|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.groupBy('Label').agg({'Length':'mean'}).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Não há grande diferença entre a mean length das críticas positivas e negativas. O próximo passo é iniciar o processo de tokenização e remover as stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remoção das  stopwords\n",
    "\n",
    "# Primeiro, Tokeniza\n",
    "tokenization=Tokenizer(inputCol='Review',outputCol='tokens')\n",
    "\n",
    "tokenized_df=tokenization.transform(text_df)\n",
    "\n",
    "# Depois remove a stopwords\n",
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')\n",
    "\n",
    "refined_text_df=stopword_removal.transform(tokenized_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como agora estamos lidando apenas com tokens em vez de uma revisão inteira, faria mais sentido capturar vários tokens em cada revisão em vez de usar o legth da revisão. Criamos outra coluna (contagem de tokens) que fornece o número de tokens em cada linha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "|              Review|Label|length|              tokens|      refined_tokens|token_count|\n",
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "|I like Harry Pott...|  1.0|    21|[i, like, harry, ...|[like, harry, pot...|          3|\n",
      "|I hate Harry Pott...|  0.0|    23|[i, hate, harry, ...|[hate, harry, pot...|          3|\n",
      "|I love Harry Pott...|  1.0|    21|[i, love, harry, ...|[love, harry, pot...|          3|\n",
      "|friday hung out w...|  0.0|    72|[friday, hung, ou...|[friday, hung, ke...|          9|\n",
      "|I love Harry Potter.|  1.0|    20|[i, love, harry, ...|[love, harry, pot...|          3|\n",
      "|i love being a se...|  1.0|    71|[i, love, being, ...|[love, sentry, mi...|          6|\n",
      "|I love The Da Vin...|  1.0|    27|[i, love, the, da...|[love, da, vinci,...|          4|\n",
      "|dudeee i LOVED br...|  1.0|    37|[dudeee, i, loved...|[dudeee, loved, b...|          4|\n",
      "|The Da Vinci Code...|  1.0|    30|[the, da, vinci, ...|[da, vinci, code,...|          4|\n",
      "|da vinci code was...|  1.0|    37|[da, vinci, code,...|[da, vinci, code,...|          5|\n",
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cria uma função aplicada a colunas no Pyspark. Ela fornece a contagem de palavras e retorna um Inteiro\n",
    "len_udf = udf(lambda s: len(s), IntegerType())\n",
    "\n",
    "\n",
    "refined_text_df = refined_text_df.withColumn(\"token_count\", len_udf(col('refined_tokens')))\n",
    "\n",
    "refined_text_df.orderBy(rand()).show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Após a etapa de stopwords precisamos converter o texto em um recurso numérico\n",
    "    - Podemos utilizar o count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "|              Review|Label|length|              tokens|      refined_tokens|token_count|\n",
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "|The Da Vinci Code...|  1.0|    39|[the, da, vinci, ...|[da, vinci, code,...|          5|\n",
      "|this was the firs...|  1.0|    72|[this, was, the, ...|[first, clive, cu...|          9|\n",
      "|i liked the Da Vi...|  1.0|    32|[i, liked, the, d...|[liked, da, vinci...|          5|\n",
      "|i liked the Da Vi...|  1.0|    32|[i, liked, the, d...|[liked, da, vinci...|          5|\n",
      "|I liked the Da Vi...|  1.0|    72|[i, liked, the, d...|[liked, da, vinci...|          8|\n",
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_text_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+-----------+----------------------------------------------------------------------------------+-----+\n",
      "|refined_tokens                                               |token_count|features                                                                          |Label|\n",
      "+-------------------------------------------------------------+-----------+----------------------------------------------------------------------------------+-----+\n",
      "|[da, vinci, code, book, awesome.]                            |5          |(2301,[0,1,4,43,236],[1.0,1.0,1.0,1.0,1.0])                                       |1.0  |\n",
      "|[first, clive, cussler, ever, read,, even, books, like, rel] |9          |(2301,[11,51,229,237,275,742,824,1087,2140],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|1.0  |\n",
      "|[liked, da, vinci, code, lot.]                               |5          |(2301,[0,1,4,53,358],[1.0,1.0,1.0,1.0,1.0])                                       |1.0  |\n",
      "|[liked, da, vinci, code, lot.]                               |5          |(2301,[0,1,4,53,358],[1.0,1.0,1.0,1.0,1.0])                                       |1.0  |\n",
      "|[liked, da, vinci, code, ultimatly, seem, hold, own.]        |8          |(2301,[0,1,4,53,656,1130,1339,1427],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])            |1.0  |\n",
      "|[even, exaggeration, ), midnight, went, wal-mart]            |6          |(2301,[46,229,271,1150,1989,2202],[1.0,1.0,1.0,1.0,1.0,1.0])                      |1.0  |\n",
      "|[loved, da, vinci, code,, want, something, better, different]|8          |(2301,[0,1,22,30,111,219,392,536],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])              |1.0  |\n",
      "|[thought, da, vinci, code, great,, kite, runner.]            |7          |(2301,[0,1,4,228,1258,1267,2262],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                   |1.0  |\n",
      "|[da, vinci, code, actually, good, movie...]                  |6          |(2301,[0,1,4,33,226,258],[1.0,1.0,1.0,1.0,1.0,1.0])                               |1.0  |\n",
      "|[thought, da, vinci, code, pretty, good, book.]              |7          |(2301,[0,1,4,223,226,228,262],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                      |1.0  |\n",
      "+-------------------------------------------------------------+-----------+----------------------------------------------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(inputCol='refined_tokens',outputCol='features')\n",
    "\n",
    "cv_text_df = count_vec.fit(refined_text_df).transform(refined_text_df)\n",
    "\n",
    "cv_text_df.select(['refined_tokens','token_count','features','Label']).show(10, False)\n",
    "\n",
    "#pag 217"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- O índice representa a pósição da palavra (coluna) única na matriz\n",
    "- Antes de atribuir um índice as palavras são ordanadas em ordem alfabetica.\n",
    "- então em vez de ter os nomes nas colunas, são atribuidos indices\n",
    "- 'da': posicao 0\n",
    "- 'vinci': posicao 1\n",
    "- 'loved': posicao zero\n",
    "\n",
    "Essas posições são definidas dentro de cada documento. Por isso existe duas palavras com a mesma posição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_text_df = cv_text_df.select(['features','token_count','Label'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assim que tivermos o vetor de recursos para cada linha, podemos usar o VectorAssembler para criar recursos de entrada para o modelo de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- token_count: integer (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      " |-- features_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_assembler = VectorAssembler(inputCols=['features', 'token_count'],outputCol='features_vec')\n",
    "\n",
    "model_text_df = df_assembler.transform(model_text_df)\n",
    "\n",
    "model_text_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos usar qualquer um dos modelos de classificação nesses dados, mas continuamos com o treinamento do Modelo de Regressão Logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df,test_df = model_text_df.randomSplit([0.75,0.25])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para validar a presença de registros suficientes para ambas as classes no conjunto de dados train e test, podemos aplicar a função groupBy na coluna Label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0| 2982|\n",
      "|  0.0| 2306|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0|  927|\n",
      "|  0.0|  775|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.groupBy('Label').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Regressão logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(featuresCol='features_vec',labelCol='Label').fit(training_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depois de treinar o modelo, avaliamos o desempenho do modelo no conjunto de dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|            features|token_count|Label|        features_vec|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|(2301,[0,1,4,5,64...|          6|  1.0|(2302,[0,1,4,5,64...|[-22.063774564624...|[2.61712494250314...|       1.0|\n",
      "|(2301,[0,1,4,5,82...|          6|  1.0|(2302,[0,1,4,5,82...|[-18.573202819923...|[8.58539933273700...|       1.0|\n",
      "|(2301,[0,1,4,10,1...|         10|  0.0|(2302,[0,1,4,10,1...|[45.9551864501695...|           [1.0,0.0]|       0.0|\n",
      "|(2301,[0,1,4,11,1...|          6|  0.0|(2302,[0,1,4,11,1...|[0.22035687262570...|[0.55486738064878...|       0.0|\n",
      "|(2301,[0,1,4,11,1...|          6|  0.0|(2302,[0,1,4,11,1...|[0.22035687262570...|[0.55486738064878...|       0.0|\n",
      "|(2301,[0,1,4,12,1...|          8|  1.0|(2302,[0,1,4,12,1...|[-10.603077352117...|[2.48388352707157...|       1.0|\n",
      "|(2301,[0,1,4,12,3...|          8|  1.0|(2302,[0,1,4,12,3...|[-21.024988852664...|[7.39542877729704...|       1.0|\n",
      "|(2301,[0,1,4,12,3...|          5|  1.0|(2302,[0,1,4,12,3...|[-22.428314702606...|[1.81763503931768...|       1.0|\n",
      "|(2301,[0,1,4,12,3...|          5|  1.0|(2302,[0,1,4,12,3...|[-22.428314702606...|[1.81763503931768...|       1.0|\n",
      "|(2301,[0,1,4,12,3...|          5|  1.0|(2302,[0,1,4,12,3...|[-22.428314702606...|[1.81763503931768...|       1.0|\n",
      "|(2301,[0,1,4,12,3...|          5|  1.0|(2302,[0,1,4,12,3...|[-22.428314702606...|[1.81763503931768...|       1.0|\n",
      "|(2301,[0,1,4,12,3...|          5|  1.0|(2302,[0,1,4,12,3...|[-22.428314702606...|[1.81763503931768...|       1.0|\n",
      "|(2301,[0,1,4,12,3...|          5|  1.0|(2302,[0,1,4,12,3...|[-22.428314702606...|[1.81763503931768...|       1.0|\n",
      "|(2301,[0,1,4,12,3...|          5|  1.0|(2302,[0,1,4,12,3...|[-22.428314702606...|[1.81763503931768...|       1.0|\n",
      "|(2301,[0,1,4,12,3...|          5|  1.0|(2302,[0,1,4,12,3...|[-22.428314702606...|[1.81763503931768...|       1.0|\n",
      "|(2301,[0,1,4,12,3...|          5|  1.0|(2302,[0,1,4,12,3...|[-22.428314702606...|[1.81763503931768...|       1.0|\n",
      "|(2301,[0,1,4,12,3...|          5|  1.0|(2302,[0,1,4,12,3...|[-22.428314702606...|[1.81763503931768...|       1.0|\n",
      "|(2301,[0,1,4,12,3...|          5|  1.0|(2302,[0,1,4,12,3...|[-22.428314702606...|[1.81763503931768...|       1.0|\n",
      "|(2301,[0,1,4,12,3...|          5|  1.0|(2302,[0,1,4,12,3...|[-22.428314702606...|[1.81763503931768...|       1.0|\n",
      "|(2301,[0,1,4,12,3...|          5|  1.0|(2302,[0,1,4,12,3...|[-22.428314702606...|[1.81763503931768...|       1.0|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = log_reg.evaluate(test_df).predictions\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VPP\n",
    "true_postives = results[(results.Label == 1) & (results.prediction == 1)].count()\n",
    "\n",
    "# VPN\n",
    "true_negatives = results[(results.Label == 0) & (results.prediction == 0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP\n",
    "false_positives = results[(results.Label == 0) & (results.prediction == 1)].count()\n",
    "\n",
    "# FN\n",
    "false_negatives = results[(results.Label == 1) & (results.prediction == 0)].count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- O desempenho do modelo parece razoavelmente bom e é capaz de diferenciar facilmente entre avaliações positivas e negativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9838187702265372\n"
     ]
    }
   ],
   "source": [
    "recall = float(true_postives)/(true_postives + false_negatives)\n",
    "\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9702127659574468\n"
     ]
    }
   ],
   "source": [
    "precision = float(true_postives) / (true_postives + false_positives)\n",
    "\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9747356051703878\n"
     ]
    }
   ],
   "source": [
    "accuracy=float((true_postives+true_negatives) /(results.count()))\n",
    "\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e33924f5cad5d014716ef47a4956958652f2e3e756f7b26cc7db08a809e4a3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
