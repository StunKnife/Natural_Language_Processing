{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Natural Language Processing com Pyspark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forçar a instalação do Pyspark para a versão 2.3.2. O motivo é que estava dando error com a versão mais recente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --force-reinstall pyspark==2.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------+\n",
      "|user_id|review                                    |\n",
      "+-------+------------------------------------------+\n",
      "|1      |I really liked this movie                 |\n",
      "|2      |I would recommend this movie to my friends|\n",
      "|3      |movie was alright but acting was horrible |\n",
      "|4      |I am never watching that movie ever again |\n",
      "+-------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenização\n",
    "df = spark.createDataFrame([(1,'I really liked this movie'),\n",
    " (2,'I would recommend this movie to my friends'),\n",
    " (3,'movie was alright but acting was horrible'),\n",
    " (4,'I am never watching that movie ever again')],\n",
    " ['user_id','review'])\n",
    "\n",
    "df.show(4,False) # False: mostra o texto completo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenização"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Temos que passar a coluna de entrada e nomear a coluna de saída após a tokenização. Usamos a função de transformação para aplicar tokenização à coluna de revisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "|user_id|review                                    |tokens                                             |\n",
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "|1      |I really liked this movie                 |[i, really, liked, this, movie]                    |\n",
      "|2      |I would recommend this movie to my friends|[i, would, recommend, this, movie, to, my, friends]|\n",
      "|3      |movie was alright but acting was horrible |[movie, was, alright, but, acting, was, horrible]  |\n",
      "|4      |I am never watching that movie ever again |[i, am, never, watching, that, movie, ever, again] |\n",
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aplica a Função de Tokenização e cria um Output como coluna\n",
    "tokenization = Tokenizer(inputCol='review',outputCol='tokens')\n",
    "tokenized_df = tokenization.transform(df)\n",
    "tokenized_df.show(4,False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Obtemos uma nova coluna denominada tokens que contém os tokens para cada frase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords Removal\n",
    "    Processo de eliminar palavras que são irrelevantes ou agregam pouco as análises. A eliminação delas promove uma melhora das exigências computacionais. Palavras como:\n",
    "        - 'this', 'the', 'to' , 'was', 'that',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "|user_id|tokens                                             |refined_tokens                    |\n",
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "|1      |[i, really, liked, this, movie]                    |[really, liked, movie]            |\n",
      "|2      |[i, would, recommend, this, movie, to, my, friends]|[recommend, movie, friends]       |\n",
      "|3      |[movie, was, alright, but, acting, was, horrible]  |[movie, alright, acting, horrible]|\n",
      "|4      |[i, am, never, watching, that, movie, ever, again] |[never, watching, movie, ever]    |\n",
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopword_removal = StopWordsRemover(inputCol='tokens', outputCol='refined_tokens')\n",
    "\n",
    "# Em seguida, passamos os tokens como a coluna de entrada e nomeamos a coluna de saída como tokens refinados.\n",
    "\n",
    "refined_df = stopword_removal.transform(tokenized_df)\n",
    "\n",
    "refined_df.select(['user_id','tokens','refined_tokens']).show(4,False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "    - Consiste em representar os dados do texto em forma numérica para que sejam utilizados por Machine Learning ou qualquer outra análise. \n",
    "    - Count Vectorizer faz a contagem da palavra que aparece no documento específico. \n",
    "    \n",
    "Desvantagem: A desvantagem de usar o método Count Vectorizer é que ele não considera as co-ocorrências de palavras em outros documentos. Em termos simples, as palavras que aparecem com mais frequência teriam um impacto maior no vetor de recursos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+--------------------------------+\n",
      "|user_id|refined_tokens                    |features                        |\n",
      "+-------+----------------------------------+--------------------------------+\n",
      "|1      |[really, liked, movie]            |(11,[0,1,9],[1.0,1.0,1.0])      |\n",
      "|2      |[recommend, movie, friends]       |(11,[0,5,10],[1.0,1.0,1.0])     |\n",
      "|3      |[movie, alright, acting, horrible]|(11,[0,2,3,4],[1.0,1.0,1.0,1.0])|\n",
      "|4      |[never, watching, movie, ever]    |(11,[0,6,7,8],[1.0,1.0,1.0,1.0])|\n",
      "+-------+----------------------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conta as palavras únicas para criação de um vetor númerico\n",
    "count_vec = CountVectorizer(inputCol='refined_tokens',outputCol='features')\n",
    "cv_df = count_vec.fit(refined_df).transform(refined_df)\n",
    "\n",
    "cv_df.select(['user_id','refined_tokens','features']).show(4,False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como podemos observar, cada sentença é representada como um vetor denso. Ele mostra que o comprimento do vetor é 11 e a primeira sentença contém 3 valores nos índices 0, 4 e 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'acting',\n",
       " 'horrible',\n",
       " 'alright',\n",
       " 'watching',\n",
       " 'ever',\n",
       " 'recommend',\n",
       " 'never',\n",
       " 'really',\n",
       " 'liked',\n",
       " 'friends']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para validar o vocabulário do vetorizador de contagem, podemos simplesmente usar a função de vocabulário.\n",
    "count_vec.fit(refined_df).vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency – Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "É outra abordagem para converter dados de texto em formato numérico.\n",
    "\n",
    "Este método tenta normalizar a frequência de ocorrência de palavras com base em outros documentos. A ideia é dar mais peso à palavra se ela aparecer mais vezes no mesmo documento, mas penalizar se ela aparecer mais vezes em outros documentos também.\n",
    "\n",
    "Isso indica que uma palavra é comum em todo o corpus e não é tão importante quanto indica sua frequência no documento atual. \n",
    "\n",
    "    - Term Frequency: Pontuação com base na frequência da palavra no documento atual. \n",
    "\n",
    "    - Inverse Document Frequency: Pontuação com base na frequência de documentos que contém a palavra atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF,IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+-------------------------------------------------------+\n",
      "|user_id|refined_tokens                    |tf_features                                            |\n",
      "+-------+----------------------------------+-------------------------------------------------------+\n",
      "|1      |[really, liked, movie]            |(262144,[99172,210223,229264],[1.0,1.0,1.0])           |\n",
      "|2      |[recommend, movie, friends]       |(262144,[68228,130047,210223],[1.0,1.0,1.0])           |\n",
      "|3      |[movie, alright, acting, horrible]|(262144,[95685,171118,210223,236263],[1.0,1.0,1.0,1.0])|\n",
      "|4      |[never, watching, movie, ever]    |(262144,[63139,113673,203802,210223],[1.0,1.0,1.0,1.0])|\n",
      "+-------+----------------------------------+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usado no mesmo dataframe df refinado.\n",
    "hashing_vec = HashingTF(inputCol='refined_tokens',outputCol='tf_features')\n",
    "\n",
    "hashing_df = hashing_vec.transform(refined_df)\n",
    "\n",
    "hashing_df.select(['user_id','refined_tokens','tf_features']).show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------------------------------------------------------------------+\n",
      "|user_id|tf_idf_features                                                                                     |\n",
      "+-------+----------------------------------------------------------------------------------------------------+\n",
      "|1      |(262144,[99172,210223,229264],[0.9162907318741551,0.0,0.9162907318741551])                          |\n",
      "|2      |(262144,[68228,130047,210223],[0.9162907318741551,0.9162907318741551,0.0])                          |\n",
      "|3      |(262144,[95685,171118,210223,236263],[0.9162907318741551,0.9162907318741551,0.0,0.9162907318741551])|\n",
      "|4      |(262144,[63139,113673,203802,210223],[0.9162907318741551,0.9162907318741551,0.9162907318741551,0.0])|\n",
      "+-------+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vec = IDF(inputCol='tf_features',outputCol='tf_idf_features')\n",
    "\n",
    "tf_idf_df = tf_idf_vec.fit(hashing_df).transform(hashing_df)\n",
    "\n",
    "tf_idf_df.select(['user_id','tf_idf_features']).show(4,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e33924f5cad5d014716ef47a4956958652f2e3e756f7b26cc7db08a809e4a3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
